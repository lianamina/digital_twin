{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5a2498d1",
      "metadata": {},
      "source": [
        "# Cerebrum Quickstart Guide\n",
        "\n",
        "This notebook demonstrates how to use Cerebrum to access an LLM via:\n",
        "- **Raw HTTP requests**\n",
        "- **Langchain integration**\n",
        "\n",
        "We'll start with a simple chat, followed by an example that uses a tool function in the conversation.\n",
        "\n",
        "See [ENVIRONMENT_SETUP.md](ENVIRONMENT_SETUP.md) for API key info, limits, and usage guidelines.\n",
        "\n",
        "*For reference, here is the CURL call:*\n",
        "\n",
        "```\n",
        "curl --location 'https://cerebrum-dev.lnkdprod.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview' \\\n",
        "  --header 'api-key: API_KEY_HERE' \\\n",
        "  --header 'Accept: application/json' \\\n",
        "  --header 'Content-Type: application/json' \\\n",
        "  --data '{\n",
        "    \"model\": \"gpt-4.1\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant.\"\n",
        "      },\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"How are you?\"\n",
        "      }\n",
        "    ]\n",
        "  }'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886decca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Chat via HTTP Request\n",
        "import requests\n",
        "import json\n",
        "\n",
        "url = 'https://cerebrum-dev.lnkdprod.com/openai/deployments/gpt-4.1/chat/completions?api-version=2024-12-01-preview'\n",
        "headers = {\n",
        "    'api-key': 'API_KEY_HERE',  # Replace with your actual API key\n",
        "    'Accept': 'application/json',\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "payload = {\n",
        "    'model': 'gpt-4.1',\n",
        "    'messages': [\n",
        "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "        {'role': 'user', 'content': 'How are you today?'}\n",
        "    ]\n",
        "}\n",
        "response = requests.post(url, headers=headers, json=payload)\n",
        "response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1418181",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Using Langchain\n",
        "Now let's integrate a simple tool function and use Cerebrum via Langchain.\n",
        "\n",
        "Let's create a function to get the LLM using Cerebrum's API. Note that we can leverage several different models, such as `gpt-4.1`, `gpt-4.0`, or `gpt-3.5-turbo`. For this example, we'll use `gpt-4.1`.\n",
        "\n",
        "*An API key is required in your environment variables.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from enum import Enum\n",
        "import httpx\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "class LLMModel(Enum):\n",
        "    GPT4_1106_PREVIEW = \"gpt-4-1106-preview\"\n",
        "    GPT35_TURBO = \"gpt-35-turbo\"\n",
        "    GPT35_TURBO_16K = \"gpt-35-turbo-16k\"\n",
        "    GPT4 = \"gpt-4\"\n",
        "    GPT4_32K = \"gpt-4-32k\"\n",
        "    GPT4_VISION_PREVIEW = \"gpt-4-vision-preview\"\n",
        "    GPT35_TURBO_1106 = \"gpt-35-turbo-1106\"\n",
        "    GPT4_0125_PREVIEW = \"gpt-4-0125-Pre view\"\n",
        "    GPT35_TURBO_0125 = \"gpt-35-turbo-0125\"\n",
        "    GPT4_TURBO = \"gpt-4-turbo\"\n",
        "    GPT4O = \"gpt-4o\"\n",
        "    GPT4O_MINI = \"gpt-4o-mini\"\n",
        "    O1_preview = \"o1-preview\"\n",
        "    O1 = \"o1\"\n",
        "    O3_MINI = \"o3-mini\"\n",
        "    GPT4_1 = \"gpt-4.1\"\n",
        "    GPT4_1_MINI = \"gpt-4.1-mini\"\n",
        "    GPT4_1_NANO = \"gpt-4.1-nano\"\n",
        "\n",
        "def get_llm(model=LLMModel.GPT4_1):\n",
        "    return AzureChatOpenAI(\n",
        "        azure_deployment=model.value,\n",
        "        api_version=\"2024-12-01-preview\",\n",
        "        api_key=os.getenv(\"CEREBRUM_KEY\"), # <------- API Key\n",
        "        azure_endpoint=\"https://cerebrum-dev.lnkdprod.com/\",\n",
        "        model=model.value,\n",
        "        model_version=\"2024-12-01-preview\",\n",
        "        http_client=httpx.Client(),\n",
        "        http_async_client=httpx.AsyncClient()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d942305",
      "metadata": {},
      "source": [
        "Langchain allows the use of tools, which are functions that the LLM can call during a conversation. In this example, we'll create a dummy tool that greets a user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool(parse_docstring=True)\n",
        "def dummy_tool(name: str) -> dict:\n",
        "    \"\"\"\n",
        "    A mock tool that returns a greeting.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the person to greet.\n",
        "    Returns:\n",
        "        dict: A greeting message.\n",
        "    \"\"\"\n",
        "    return {\"message\": f\"Hello, {name}!\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e926ff2",
      "metadata": {},
      "source": [
        "Now let's put it all together. We can bind the tool to the LLM and invoke it in a conversation. Notice that we tell the LLM to use the tool in the user message in plain language, and the LLM will call the tool when appropriate.\n",
        "\n",
        "We can then see the result of the tool invocation in the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = get_llm()\n",
        "llm_with_tools = llm.bind_tools([dummy_tool])\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Greet Adrian using the dummy tool.\"}\n",
        "]\n",
        "\n",
        "response = llm_with_tools.invoke(messages)\n",
        "tool_call = response.tool_calls[0]\n",
        "tool_args = tool_call['args']\n",
        "tool_call_id = tool_call['id']\n",
        "\n",
        "tool_output = dummy_tool.invoke(tool_args)\n",
        "\n",
        "followup_messages = messages + [\n",
        "    response,\n",
        "    ToolMessage(tool_call_id=tool_call_id, content=str(tool_output))\n",
        "]\n",
        "\n",
        "final_response = llm_with_tools.invoke(followup_messages)\n",
        "print(\"Final response:\", final_response.content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
